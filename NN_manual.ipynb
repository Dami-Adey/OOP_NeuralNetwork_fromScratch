{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP865WNV3nx3cEZ8cJ7MjSa"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Neural Network from Scratch Using Object-Oriented Programming (OOP)\n",
        "\n",
        "In this notebook I develop an object-oriented sequential neural network from scratch as a personal project. The main motivator for this work is a more rigorous application of concepts learned in Andrew Ng's machine learning courses:\n",
        "\n",
        "- [Machine Learning Specialisation](https://www.coursera.org/specializations/machine-learning-introduction)\n",
        "- [Deep Learning Specialisation](https://www.coursera.org/specializations/deep-learning)\n",
        "\n",
        "\n",
        "The full implementation will have two main components:\n",
        "\n",
        "\n",
        "1.   A neural network layer constructor that allows the user to define layer properties similarly to the Keras API for TensorFlow.\n",
        "2.   A model compiler that takes a series of layers and forms the neural network.\n",
        "\n"
      ],
      "metadata": {
        "id": "wZkfvKDCMmVW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iY56k7IVWfPg"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network Layer Object\n",
        "\n",
        "The neural network layer object is created below. Although the goal is mimic Keras' ease of use through OOP, the features included in this build are not an exhaustive reproduction of Keras' feature list, since that is well beyond the scope of this work. The following functionality  is included in the current version:\n",
        "- Generalised object architechture allows the user to initialise a layer with\n",
        "  - a number of units/neurons\n",
        "  - an activation function\n",
        "  - weight and bias parameters\n",
        "  - a layer id/name\n",
        "- A <u>print()</u> call produces a brief summary of the layer properties.\n",
        "- The <u>initialiseWB()</u> method initialises the layer weights and biases based on an expected input.\n",
        "- Once the model has been initialised for an input, the layer object validates any future inputs using the <u>is_consistent()</u> method.\n",
        "- The <u>get_parameters()</u> and <u>set_parameters()</u> methods allow for the inspection and  update of the layers weights and biases to facilitate the learning mechanisms.\n",
        "- The <u>activate()</u> method computes the local forward propagation step (vectorised) using one of the following activation functions:\n",
        "  - Linear (i.e, no activation function)\n",
        "  - Sigmoid\n",
        "  - Hyperbolic Tangent Function (tanh)\n",
        "  - Softmax\n",
        "  - Rectified Linear Unit (ReLU)\n"
      ],
      "metadata": {
        "id": "jjlAPoxQimtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NN_DenseLayer():\n",
        "  # class variables for scaling initial weights/biases\n",
        "  WB_SCALE = 0.01\n",
        "\n",
        "  def __init__(self,units,activation='sigmoid',name=None,W_init=None,B_init=None):\n",
        "    # initialise layer data\n",
        "    self.neurons = units          # number of neurons in the layer\n",
        "    self.activation = activation  # activation function applied at the layer\n",
        "    self.name = name              # layer name (if desired)\n",
        "\n",
        "    # features will be updated when an input is passed to the layer\n",
        "    self.features = None\n",
        "\n",
        "    # initialise layer weights and biases (inactive by default)\n",
        "    self.wb_init = False\n",
        "    self.W = W_init\n",
        "    self.B = B_init\n",
        "\n",
        "    # input sanitation parameter\n",
        "    self.expectedInputShape = None\n",
        "\n",
        "\n",
        "  def __str__(self):\n",
        "    '''\n",
        "      Layer print protocol.\n",
        "    '''\n",
        "    # print dialogue\n",
        "    return f\"\\nNeural-network layer (dense) with {self.neurons} neurons. {self.activation.title()} activation. \\n\"\n",
        "\n",
        "\n",
        "  def initialiseWB(self,A_in,featureCount,neuronCount):\n",
        "    '''\n",
        "    Estimates initial values for weight and bias arrays:\n",
        "\n",
        "    WEIGHT INITIALISATION PROTOCOL:\n",
        "    --------------------------------------------------------------------------\n",
        "      - the total number of elements in the weight matrix, W_elem = n*j\n",
        "      - create an array with elements totaling W_elem\n",
        "      - then reshape array to the desired (n,j) and scale the values using WB_INIT\n",
        "\n",
        "    BIAS INITIALISATION PROTOCOL:\n",
        "    --------------------------------------------------------------------------\n",
        "      - the total number of elements in the bias vector, B_elem = j\n",
        "      - create an array with elements totaling B_elem\n",
        "      - reshape to the desired (j,) and scale the values using WB_INIT\n",
        "    '''\n",
        "    # find total elements in weight matrix\n",
        "    W_elem = featureCount*neuronCount  # n*j\n",
        "    # reshape weight array, and scale\n",
        "    weight_arr = self.WB_SCALE * np.arange(1,W_elem+1,1).reshape(featureCount,neuronCount)\n",
        "\n",
        "    # total bias vector elements\n",
        "    B_elem = neuronCount  # j\n",
        "    # reshape bias array, and scale\n",
        "    bias_arr = self.WB_SCALE * np.arange(1,B_elem+1,1).reshape(neuronCount, 1)\n",
        "    # NOTE^: a 1D vector is fine for B since python will automatically 'brodacast'\n",
        "    # the values correctly to align with the shape of A_in*W.\n",
        "\n",
        "    return weight_arr, bias_arr\n",
        "\n",
        "\n",
        "  def is_consistent(self,A_in):\n",
        "    '''\n",
        "    Checks if the input A_in is consistent with previous inputs to the layer.\n",
        "    '''\n",
        "    # returns true if the input shape is consistent with the previous iterations\n",
        "    if A_in.shape == self.expectedInputShape:\n",
        "      return True\n",
        "    # if a new input format is detected, update the expected input shape to match the new input\n",
        "    self.expectedInputShape = A_in.shape\n",
        "    return False\n",
        "\n",
        "\n",
        "  def count_features(self,A_in):\n",
        "    '''\n",
        "    Counts the number of features, n, present in the input A_in\n",
        "    '''\n",
        "    try:\n",
        "      # if A_in is a matrix, n is the number of columns, index 1\n",
        "      numFeatures = A_in.shape[1]\n",
        "    except IndexError:\n",
        "      # an IndexError above would suggest that A_in is a 1D array. Therefore n is index 0\n",
        "      numFeatures = A_in.shape[0]\n",
        "\n",
        "    return numFeatures\n",
        "\n",
        "\n",
        "  def activate(self, A_in):\n",
        "    '''\n",
        "      Evaluates the layer output(s) for an input A_in:\n",
        "\n",
        "      A_in -  input data    (m,n)  |  m examples with n features each\n",
        "      W    -  layer weights (n,j)  |  n features per neuron, j neurons/units\n",
        "      B    -  bias vector   (1,j)  |  j neurons/units\n",
        "    '''\n",
        "    self.current_input = A_in\n",
        "    # check if the layer was expecting an input of this format\n",
        "    if self.is_consistent(self.current_input):\n",
        "      pass # do nothing if input was expected in this format\n",
        "    else:\n",
        "      # count the number of features in the new input format\n",
        "      self.features = self.count_features(self.current_input)\n",
        "      # initialise weight/bias arrays for the new input\n",
        "      self.W, self.B = self.initialiseWB(self.current_input,self.features,self.neurons)\n",
        "      # indicate that weights/biases have been initialised\n",
        "      self.wb_init = True\n",
        "\n",
        "    # if weights/biases are not initialised, then estimate them.\n",
        "    if not self.wb_init:\n",
        "      # estimate inital values for W and B\n",
        "      self.W, self.B = self.initialiseWB(self.current_input,self.features,self.neurons)\n",
        "      # the layer is now initialised\n",
        "      self.wb_init = True\n",
        "\n",
        "    # apply weights an biases to inputs; (z = A_in*W + B)\n",
        "    z = np.matmul(self.current_input, self.W) + self.B\n",
        "\n",
        "    # apply the chosen activation function\n",
        "    # 1.0 sigmoid\n",
        "    if self.activation.lower() == 'linear':\n",
        "      self.out = self.linear(z)\n",
        "    elif self.activation.lower() == 'sigmoid':\n",
        "      self.out = self.sigmoid(z)\n",
        "    elif self.activation.lower() == 'relu':\n",
        "      self.out = self.relu(z)\n",
        "    else:\n",
        "      self.out = None\n",
        "      raise Exception(\"Invalid activation function.\")\n",
        "\n",
        "    return self.out\n",
        "\n",
        "\n",
        "  def get_parameters(self):\n",
        "    '''\n",
        "      Returns the current weights and biases for the layer\n",
        "    '''\n",
        "    return [self.W, self.B]\n",
        "\n",
        "\n",
        "  def set_parameters(self,W_set,B_set):\n",
        "    '''\n",
        "      Overwrites the weights and biases of the current layer\n",
        "    '''\n",
        "    try:\n",
        "      # if setpoint values are null, then W/B are set to None\n",
        "      if (W_set == None) and (B_set == None):\n",
        "        self.W = W_set\n",
        "        self.B = B_set\n",
        "        # indicate that the weights/biases have been nullified\n",
        "        self.wb_init = False\n",
        "\n",
        "    except ValueError:\n",
        "      if (self.W == None) or (self.B == None):\n",
        "        # creat proxy weights/biases using the current input structure\n",
        "        W_proxy, B_proxy = self.initialiseWB(self.current_input,self.features,self.neurons)\n",
        "      else:\n",
        "        W_proxy = self.W\n",
        "        B_proxy = self.B\n",
        "\n",
        "      # if non-null, the shape of the weight and bias structures should be consistent\n",
        "      if (W_set.shape == W_proxy.shape) and (B_set.shape == B_proxy.shape):\n",
        "        self.W = W_set\n",
        "        self.B = B_set\n",
        "      # if not, the W/B structures chosen are incompatible\n",
        "      else:\n",
        "        return print(f\"\\nINVALID COMMAND(!): \\n\\nLayer is only compatible with a {W_proxy.shape} weight matrix, and {B_proxy.shape} bias vector\")\n",
        "\n",
        "\n",
        "  #----------------------------------------------------------------------------#\n",
        "  # ACTIVATION FUNCTIONS\n",
        "  #----------------------------------------------------------------------------#\n",
        "  # Linear function (regression problems; negative or positive)\n",
        "  def linear(self,z):\n",
        "    '''\n",
        "      Evaluates the linear function g on an input z\n",
        "\n",
        "      Such that, g(z) = z\n",
        "    '''\n",
        "    return z\n",
        "\n",
        "\n",
        "  # Sigmoid function (binary classification problems; 1 or 0, etc.)\n",
        "  def sigmoid(self,z):\n",
        "    '''\n",
        "      Evaluates the sigmoid function g on an input z\n",
        "\n",
        "      Where, g(z) =      1\n",
        "                     ----------\n",
        "                     1 + e^(-z)\n",
        "    '''\n",
        "    return 1/(1 + np.exp(-z))\n",
        "\n",
        "\n",
        "  # Hyperbolic Tangent function (alternative to sigmoid, varies between -1 an 1 instead of 0 and 1)\n",
        "  def tanh(self,z):\n",
        "    '''\n",
        "      Evaluates the Hyperbolic Tangent (tanh) function g on an input z\n",
        "\n",
        "      Where, g(z) =    e^(z) - e^(-z)\n",
        "                       --------------\n",
        "                       e^(z) + e^(-z)\n",
        "    '''\n",
        "    return np.tanh(z)\n",
        "\n",
        "\n",
        "  # ReLU function (regression problems; threshold utility; non-negative values only)\n",
        "  def relu(self,z):\n",
        "    '''\n",
        "      Evaluates the Rectified Linear Unit (ReLU) function g on an input z\n",
        "\n",
        "      Where, g(z) = 0, if (z < 0)\n",
        "             g(z) = z, if (z > 0)\n",
        "    '''\n",
        "    return max(0,z)\n",
        "\n",
        "\n",
        "  # Softmax activation (multi-class classification, i.e. mutiple dicrete choices, one correct choice per example)\n",
        "  def softmax(self,z):\n",
        "    '''\n",
        "    Evaluates the softmax activation function g on an input z. Softmax converts\n",
        "    z for all possible choice into a distribution of probabilities that each\n",
        "    choice is correct.\n",
        "\n",
        "    For a problem with N choices, the probability that the kth choice is\n",
        "    correct is represented by:\n",
        "\n",
        "           g(z_{i,j,k,...,N}) =         e^z_k\n",
        "                                 -------------------\n",
        "                                 e^z_i + ... + e^z_N\n",
        "    '''\n",
        "    return np.exp(z)/np.sum(np.exp(z))\n"
      ],
      "metadata": {
        "id": "Z5D0zYRqXQix"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function Tests - Layer Behaviour\n",
        "\n",
        "Below are some tests to ensure that the neural network layer produces the expected behaviour."
      ],
      "metadata": {
        "id": "sXRMB-A7YJjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new dense layer\n",
        "L1 = NN_DenseLayer(3,'Sigmoid','layer1')"
      ],
      "metadata": {
        "id": "5_dkXBaq2Eva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print new layer\n",
        "print(L1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZGwoTaaJy8m",
        "outputId": "dc24e500-08a4-43c1-c6fe-2fc5270e861e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Neural-network layer (dense) with 3 neurons.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check layer weights\n",
        "L1.get_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvddf9FfJy4e",
        "outputId": "b0c7f81d-eb5d-4f4f-c22d-1b139047a6ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None]"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the expected input shape\n",
        "L1.expectedInputShape"
      ],
      "metadata": {
        "id": "gSWzbGtuhv0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# activate the layer by passing an array input\n",
        "test_in = np.array([1,2,3])\n",
        "L1.activate(test_in)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buRdiShgJywe",
        "outputId": "8653ff17-1c92-4c92-91b5-00c7aac809f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.95689275, 0.97811873, 0.98901306])"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The expected input shape should now match the most recent input (1D array)\n",
        "L1.expectedInputShape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqOo470Qh3ch",
        "outputId": "369e793f-beef-4f39-c117-ff5603bbe96e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3,)"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check layer weights\n",
        "L1.get_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBq34AF2Y3wp",
        "outputId": "79cd7170-b4a0-4eeb-cec3-02b452b65190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[0.1, 0.2, 0.3],\n",
              "        [0.4, 0.5, 0.6],\n",
              "        [0.7, 0.8, 0.9]]),\n",
              " array([0.1, 0.2, 0.3])]"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reset weights\n",
        "W = None\n",
        "B = None\n",
        "L1.set_parameters(W, B)\n",
        "L1.get_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JPVVEb1ZFWt",
        "outputId": "f6a2fc4d-fe1a-4591-bbf3-a5fe3bddf4cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None]"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set invalid weights\n",
        "W = np.arange(1,3,1)\n",
        "B = np.arange(1,10,2)\n",
        "L1.set_parameters(W, B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-R5OBmD8Zuad",
        "outputId": "a8515fbd-3f8b-4d9d-b96e-ded6d65f9a15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "INVALID COMMAND(!): \n",
            "\n",
            "Layer is only compatible with a (3, 3) weight matrix, and (3,) bias vector\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check that layer weights were NOT changed\n",
        "L1.get_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZVmLEXpgtvR",
        "outputId": "f0123e2c-2ba8-4bbc-8f75-c469d2b618bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None]"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# activate for a new input (matrix type)\n",
        "new_in = np.array([[1,2,3],[4,5,6]])\n",
        "L1.activate(new_in)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdCd89_cgwSy",
        "outputId": "662b988b-5f0e-466e-9e69-fbbc2f32d533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.95689275, 0.97811873, 0.98901306],\n",
              "       [0.9987706 , 0.99975154, 0.99994983]])"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check layer weights\n",
        "L1.get_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17e_1u2QhDOS",
        "outputId": "1c105dfa-06f7-4873-b7b8-0f6ff891c5b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[0.1, 0.2, 0.3],\n",
              "        [0.4, 0.5, 0.6],\n",
              "        [0.7, 0.8, 0.9]]),\n",
              " array([0.1, 0.2, 0.3])]"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The expected input should update to match the new input type (2D matrix)\n",
        "L1.expectedInputShape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUvaNUJ1hOL7",
        "outputId": "c19c88b1-fcd0-4f75-823f-1eb66ba896b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network Model Compiler"
      ],
      "metadata": {
        "id": "Kw-guNhXiujX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NN_Model:\n",
        "  def __init__(self,layers):\n",
        "    # assign layer identifiers for the NN\n",
        "    self.layers = layers\n",
        "    self.num_layers = len(self.layers)\n",
        "    self.current_input = None\n",
        "\n",
        "\n",
        "  def summarise(self):\n",
        "    '''\n",
        "    Summarises the features of the Neural Netwrok instance\n",
        "    '''\n",
        "    print(f\"{len(self.layers)} Layer Neural-Network (Sequential):\")\n",
        "    print(f\"------------------------------------\\n\")\n",
        "    for id in range(len(self.layers)):\n",
        "      print(f\"Layer {id + 1}\")\n",
        "      print(self.layers[id])\n",
        "\n",
        "\n",
        "  def compile(self):\n",
        "    pass\n",
        "\n",
        "\n",
        "  def fit(self):\n",
        "    pass\n",
        "\n",
        "\n",
        "  def forward_propagation(self):\n",
        "    pass\n",
        "\n",
        "\n",
        "  def backward_propagation(self):\n",
        "    pass\n",
        "\n",
        "\n",
        "  def gradient_descent(self):\n",
        "    pass\n",
        "\n",
        "\n",
        "  def update_parameters(self):\n",
        "    pass\n",
        "\n",
        "\n",
        "  def predict(self):\n",
        "    pass\n",
        "\n"
      ],
      "metadata": {
        "id": "8D2pILctiZ5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unit Tests - Sequential NN Behaviour"
      ],
      "metadata": {
        "id": "1Th5Qq6Dmfq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create NN layer objects\n",
        "L1 = NN_DenseLayer(25,'sigmoid','layer1')\n",
        "L2 = NN_DenseLayer(15,'sigmoid','layer2')\n",
        "L3 = NN_DenseLayer(1,'sigmoid','layer3')\n",
        "\n",
        "# pass layers to NN sequantial constructor model\n",
        "model = NN_Model([L1,L2,L3])"
      ],
      "metadata": {
        "id": "pHRMZhZ4icuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "model.summarise()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXlogTcimlsI",
        "outputId": "a4fded27-7b16-4795-af21-b1fc746f361c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 Layer Neural-Network (Sequential):\n",
            "------------------------------------\n",
            "\n",
            "Layer 1\n",
            "\n",
            "Neural-network layer (dense) with 25 neurons. Sigmoid activation. \n",
            "\n",
            "Layer 2\n",
            "\n",
            "Neural-network layer (dense) with 15 neurons. Sigmoid activation. \n",
            "\n",
            "Layer 3\n",
            "\n",
            "Neural-network layer (dense) with 1 neurons. Sigmoid activation. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5PnP53UkLWJd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}