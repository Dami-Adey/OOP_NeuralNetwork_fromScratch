{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOo4j3zUiOlr4D/WvOlvL+y"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Neural Network from Scratch Using Object-Oriented Programming (OOP)\n",
        "\n",
        "In this notebook I develop an object-oriented sequential neural network from scratch as a personal project. The main motivator for this work is a more rigorous application of concepts learned in Andrew Ng's machine learning courses:\n",
        "\n",
        "- [Machine Learning Specialisation](https://www.coursera.org/specializations/machine-learning-introduction)\n",
        "- [Deep Learning Specialisation](https://www.coursera.org/specializations/deep-learning)\n",
        "\n",
        "\n",
        "The full implementation will have two main components:\n",
        "\n",
        "\n",
        "1.   A neural network layer constructor that allows the user to define layer properties similarly to the Keras API for TensorFlow.\n",
        "2.   A model compiler that takes a series of layers and forms the neural network.\n",
        "\n"
      ],
      "metadata": {
        "id": "wZkfvKDCMmVW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iY56k7IVWfPg"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "import numpy as np\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network Layer Object\n",
        "\n",
        "The neural network layer object is created below. While the goal is mimic Keras' ease of use through OOP, the features included in this build are not an exhaustive reproduction of Keras' feature list, since that is well beyond the scope of this work. The following functionality  is included in the current version:\n",
        "- Generalised object architechture allows the user to initialise a layer with\n",
        "  - a number of units/neurons\n",
        "  - an activation function\n",
        "  - weight and bias parameters\n",
        "  - a layer id/name\n",
        "- A <u>print()</u> call produces a brief summary of the layer properties.\n",
        "- The <u>initialiseWB()</u> method initialises the layer weights and biases based on an expected input.\n",
        "- Once the model has been initialised for an input, the layer object validates any future inputs using the <u>is_consistent()</u> method.\n",
        "- The <u>get_parameters()</u> and <u>set_parameters()</u> methods allow for the inspection and  update of the layers weights and biases to facilitate the learning mechanisms.\n",
        "- The <u>forward()</u> method computes the local forward propagation step (vectorised) using one of the following activation functions:\n",
        "  - Linear (i.e, no activation function)\n",
        "  - Sigmoid\n",
        "  - Hyperbolic Tangent Function (tanh)\n",
        "  - Softmax\n",
        "  - Rectified Linear Unit (ReLU)\n"
      ],
      "metadata": {
        "id": "jjlAPoxQimtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NN_DenseLayer():\n",
        "  # class variables for scaling initial weights/biases\n",
        "  WB_SCALE = 0.01\n",
        "\n",
        "  def __init__(self,units,activation='sigmoid',name=None,W_init=None,B_init=None):\n",
        "    # initialise layer data\n",
        "    self.neurons = units          # number of neurons in the layer\n",
        "    self.activation = activation  # activation function applied at the layer\n",
        "    self.name = name              # layer name (if desired)\n",
        "\n",
        "    # features will be updated when an input is passed to the layer\n",
        "    self.features = None\n",
        "\n",
        "    # initialise layer weights and biases (inactive by default)\n",
        "    self.wb_initialised = False\n",
        "    self.W = W_init\n",
        "    self.B = B_init\n",
        "\n",
        "    # input sanitation parameter\n",
        "    self.expectedInputShape = None\n",
        "\n",
        "\n",
        "  def __str__(self):\n",
        "    '''\n",
        "      Prints a brief summary of the layer properties when print() is called.\n",
        "    '''\n",
        "    # print dialogue\n",
        "    return f\"\\nNeural-network layer (dense) with {self.neurons} neurons. {self.activation.title()} activation. \\n\"\n",
        "\n",
        "\n",
        "  def initialiseWB(self,featureCount,neuronCount):\n",
        "    '''\n",
        "      Produces initial values for weight and bias arrays:\n",
        "\n",
        "      PARAMETER INITIALISATION PROTOCOL:\n",
        "      --------------------------------------------------------------------------\n",
        "        - initialise the weights matrix to the desired (n,j), populating with a random sample from a normal distributiion\n",
        "        - initialise the bias matrix to the desired (1,j), populating with zeros\n",
        "        - scale both values using WB_SCALE\n",
        "    '''\n",
        "    # initialise weight array, and scale\n",
        "    weights_matrix = np.random.randn(featureCount,neuronCount) * self.WB_SCALE\n",
        "\n",
        "    # initialise bias array, and scale\n",
        "    bias_matrix = np.zeros((1,neuronCount)) * self.WB_SCALE\n",
        "\n",
        "    # indicate that weights/biases have been initialised\n",
        "    self.wb_initialised = True\n",
        "\n",
        "    return weights_matrix, bias_matrix\n",
        "\n",
        "\n",
        "  def is_consistent(self,X_in):\n",
        "    '''\n",
        "    Checks if the input X_in is consistent with previous inputs to the layer.\n",
        "    '''\n",
        "    # returns true if the input shape is consistent with the previous iterations\n",
        "    if X_in.shape == self.expectedInputShape:\n",
        "      return True\n",
        "    return False\n",
        "\n",
        "\n",
        "  def update_input(self, X_in):\n",
        "    '''\n",
        "      Updates the expected input shape for the layer based on the structure of a new input X_in\n",
        "    '''\n",
        "    # update the expected input shape to match X_in\n",
        "    self.expectedInputShape = X_in.shape\n",
        "\n",
        "\n",
        "\n",
        "  def count_features(self,X_in):\n",
        "    '''\n",
        "    Counts the number of features, n, present in the input X_in\n",
        "    '''\n",
        "    try:\n",
        "      # if X_in is a matrix, n is the number of columns, index 1\n",
        "      numFeatures = X_in.shape[1]\n",
        "    except IndexError:\n",
        "      # raise an error if the input is a 1D array\n",
        "      raise Exception('Input must be a 2D matrix.')\n",
        "\n",
        "    return numFeatures\n",
        "\n",
        "\n",
        "  def forward(self, X_in):\n",
        "    '''\n",
        "      Evaluates the layer output(s) for an input X_in:\n",
        "\n",
        "      X_in -  input data    (m,n)  |  m examples with n features each\n",
        "      W    -  layer weights (n,j)  |  n features per neuron for j neurons/units\n",
        "      B    -  bias vector   (1,j)  |  j neurons/units\n",
        "    '''\n",
        "    self.current_input = copy.deepcopy(X_in)\n",
        "    # check if the layer was expecting an input of this format\n",
        "    if self.is_consistent(self.current_input):\n",
        "      pass # do nothing if input was expected in this format\n",
        "    else:\n",
        "      # we're dealing with a new input. so update the expected input shape\n",
        "      self.update_input(self.current_input)\n",
        "      # count the number of features in the new input format\n",
        "      self.features = self.count_features(self.current_input)\n",
        "      # initialise weight/bias arrays for the new input\n",
        "      self.W, self.B = self.initialiseWB(self.features,self.neurons)\n",
        "\n",
        "    # if weights/biases have not yet been initialised, then do so.\n",
        "    if not self.wb_initialised:\n",
        "      # estimate inital values for W and B\n",
        "      self.W, self.B = self.initialiseWB(self.features,self.neurons)\n",
        "\n",
        "    # apply linear function using weights an biases on the layer input; (z = X_in*W + B)\n",
        "    z = np.matmul(self.current_input, self.W) + self.B\n",
        "\n",
        "    # library of built-in activation functions\n",
        "    forward_activations = {\n",
        "        'linear':self.linear(z,'forward'),\n",
        "        'sigmoid':self.sigmoid(z,'forward'),\n",
        "        'tanh':self.tanh(z,'forward'),\n",
        "        'softmax':self.softmax(z,'forward'),\n",
        "        'relu':self.relu(z,'forward'),\n",
        "        'prelu':self.prelu(z,'forward')\n",
        "    }\n",
        "\n",
        "    # attempt to apply the chosen activation function\n",
        "    try:\n",
        "      # pull the relevant activation output\n",
        "      self.out = forward_activations[self.activation.lower()]\n",
        "    except KeyError:\n",
        "      # activation call failed\n",
        "      self.out = None\n",
        "      # find valid activation functions\n",
        "      valid_activation_keys = [key for (key, values) in forward_activations.items()]\n",
        "      # throw exception and recommend valid activation functions\n",
        "      raise Exception(f\"Invalid activation function. Pick one of the following: {valid_activation_keys}\")\n",
        "\n",
        "    # cache input values for backward propagation step\n",
        "    linear_inputs = (self.current_input, self.W, self.B)\n",
        "    activation_inputs = (z)\n",
        "    cache = (linear_inputs, activation_inputs)\n",
        "\n",
        "    return self.out, cache\n",
        "\n",
        "\n",
        "  def get_parameters(self):\n",
        "    '''\n",
        "      Returns the current weights and biases for the layer\n",
        "    '''\n",
        "    # package parameters into a dictionary and return\n",
        "    return {'W':self.W, 'B':self.B}\n",
        "\n",
        "\n",
        "  def set_parameters(self,W_set,B_set):\n",
        "    '''\n",
        "      Overwrites the weights and biases of the current layer\n",
        "    '''\n",
        "    try:\n",
        "      # if setpoint values are null, then W/B are set to None\n",
        "      if (W_set == None) and (B_set == None):\n",
        "        self.W = copy.deepcopy(W_set)\n",
        "        self.B = copy.deepcopy(B_set)\n",
        "        # indicate that the weights/biases have been nullified\n",
        "        self.wb_initialised = False\n",
        "\n",
        "    except ValueError:\n",
        "      if (self.W == None) or (self.B == None):\n",
        "        # create proxy weights/biases using the current input structure\n",
        "        W_proxy, B_proxy = self.initialiseWB(self.features,self.neurons)\n",
        "      else:\n",
        "        W_proxy = copy.deepcopy(self.W)\n",
        "        B_proxy = copy.deepcopy(self.B)\n",
        "\n",
        "      # if non-null, the shape of the weight and bias structures should be consistent\n",
        "      if (W_set.shape == W_proxy.shape) and (B_set.shape == B_proxy.shape):\n",
        "        self.W = copy.deepcopy(W_set)\n",
        "        self.B = copy.deepcopy(B_set)\n",
        "      # if not, the W/B structures chosen are incompatible\n",
        "      else:\n",
        "        return print(f\"\\nINVALID COMMAND(!): \\n\\nLayer is only compatible with a {W_proxy.shape} weight matrix, and {B_proxy.shape} bias matrix\")\n",
        "\n",
        "\n",
        "  #----------------------------------------------------------------------------#\n",
        "  # ACTIVATION FUNCTIONS\n",
        "  #----------------------------------------------------------------------------#\n",
        "  # Linear function (regression problems; negative or positive)\n",
        "  def linear(self,z,direction='forward'):\n",
        "    '''\n",
        "      Evaluates the linear function g(z) or its derivative g'(z) on the input z\n",
        "\n",
        "      Where,    g(z) = z\n",
        "\n",
        "      And,      g'(z) = 1\n",
        "    '''\n",
        "    # during forward propagation...\n",
        "    if direction.lower() == 'forward':\n",
        "      # return g(z)\n",
        "      return copy.deepcopy(z)\n",
        "\n",
        "    # during backward propagation...\n",
        "    elif direction.lower() == 'backward':\n",
        "      # return g'(z)\n",
        "      return np.ones(z.shape)\n",
        "\n",
        "    else:\n",
        "      # otherwise, indicate invalid method call\n",
        "      raise Exception('Invalid acivation direction. Must be \"forward\" or \"backward\"')\n",
        "\n",
        "\n",
        "  # Sigmoid function (binary classification problems; 1 or 0, etc.)\n",
        "  def sigmoid(self,z,direction='forward'):\n",
        "    '''\n",
        "      Evaluates the sigmoid function g(z) or its derivative g'(z) on the input z\n",
        "\n",
        "      Where, g(z) =      1\n",
        "                     ----------\n",
        "                     1 + e^(-z)\n",
        "\n",
        "      And,   g'(z) = g(z) * (1 - g(z))\n",
        "    '''\n",
        "\n",
        "    # during forward propagation...\n",
        "    if direction.lower() == 'forward':\n",
        "      # return g(z)\n",
        "      return 1/(1 + np.exp(-z))\n",
        "\n",
        "    # during backward propagation...\n",
        "    elif direction.lower() == 'backward':\n",
        "      # return g'(z)\n",
        "      g_z = 1/(1 + np.exp(-z))\n",
        "      return (g_z) * (1 - (g_z))\n",
        "\n",
        "    else:\n",
        "      # otherwise, indicate invalid method call\n",
        "      raise Exception('Invalid acivation direction. Must be \"forward\" or \"backward\"')\n",
        "\n",
        "\n",
        "\n",
        "  # Hyperbolic Tangent function (alternative to sigmoid, varies between -1 an 1 instead of 0 and 1)\n",
        "  def tanh(self,z,direction='forward'):\n",
        "    '''\n",
        "      Evaluates the Hyperbolic Tangent (tanh) function g or its derivative g' on an input z\n",
        "\n",
        "      Where, g(z) =    e^(z) - e^(-z)\n",
        "                       --------------\n",
        "                       e^(z) + e^(-z)\n",
        "\n",
        "\n",
        "      And,   g'(z) =   1 - (g(z))^2\n",
        "    '''\n",
        "\n",
        "    # during forward propagation...\n",
        "    if direction.lower() == 'forward':\n",
        "      # return g(z)\n",
        "      return np.tanh(z)\n",
        "\n",
        "    # during backward propagation...\n",
        "    elif direction.lower() == 'backward':\n",
        "      # return g'(z)\n",
        "      return 1 - np.tanh(z)**2\n",
        "\n",
        "    else:\n",
        "      # otherwise, indicate invalid method call\n",
        "      raise Exception('Invalid acivation direction. Must be \"forward\" or \"backward\"')\n",
        "\n",
        "\n",
        "\n",
        "  # ReLU function (regression problems; threshold utility; non-negative values only)\n",
        "  def relu(self,z,direction='forward'):\n",
        "    '''\n",
        "      Evaluates the Rectified Linear Unit (ReLU) function g or its derivative g' on an input z\n",
        "\n",
        "      Where, g(z) = 0, if (z < 0)\n",
        "             g(z) = z, if (z >= 0)\n",
        "\n",
        "      And,   g'(z) = 0 if (z < 0)\n",
        "             g'(z) = 1 if (z >= 0)\n",
        "    '''\n",
        "\n",
        "    # during forward propagation...\n",
        "    if direction.lower() == 'forward':\n",
        "      # return g(z)\n",
        "      return np.maximum(0,z)\n",
        "\n",
        "    # during backward propagation...\n",
        "    elif direction.lower() == 'backward':\n",
        "      # return g'(z)\n",
        "      return (z >= 0) * 1\n",
        "\n",
        "    else:\n",
        "      # otherwise, indicate invalid method call\n",
        "      raise Exception('Invalid acivation direction. Must be \"forward\" or \"backward\"')\n",
        "\n",
        "\n",
        "  # Parametric ReLU function\n",
        "  def prelu(self,z,direction='forward',scalar=0.01):\n",
        "    '''\n",
        "      Evaluates the Rectified Linear Unit (ReLU) function g or its derivative g' on an input z\n",
        "\n",
        "      Where, g(z) = 0, if (z < 0)\n",
        "             g(z) = z, if (z >= 0)\n",
        "\n",
        "      And,   g'(z) = 0 if (z < 0)\n",
        "             g'(z) = 1 if (z >= 0)\n",
        "    '''\n",
        "\n",
        "    # during forward propagation...\n",
        "    if direction.lower() == 'forward':\n",
        "      # return g(z)\n",
        "      return np.maximum(scalar*z, z)\n",
        "\n",
        "    # during backward propagation...\n",
        "    elif direction.lower() == 'backward':\n",
        "      # return g'(z)\n",
        "      return (z >= 0) * scalar\n",
        "\n",
        "    else:\n",
        "      # otherwise, indicate invalid method call\n",
        "      raise Exception('Invalid acivation direction. Must be \"forward\" or \"backward\"')\n",
        "\n",
        "\n",
        "  # Softmax activation (multi-class classification, i.e. mutiple dicrete choices, one correct choice per example)\n",
        "  def softmax(self,z,direction='forward'):\n",
        "    '''\n",
        "    Evaluates the softmax activation function g or its derivative g' on an input z. Softmax converts\n",
        "    z for all possible choice into a distribution of probabilities that each\n",
        "    choice is correct.\n",
        "\n",
        "    For a problem with N choices, the probability that the kth choice is\n",
        "    correct is represented by:\n",
        "\n",
        "           g(z_{i,j,k,...,N}) =         e^z_k\n",
        "                                 -------------------\n",
        "                                 e^z_i + ... + e^z_N\n",
        "    '''\n",
        "\n",
        "    # during forward propagation...\n",
        "    if direction.lower() == 'forward':\n",
        "      # return g(z)\n",
        "      return np.exp(z)/np.sum(np.exp(z))\n",
        "\n",
        "    # during backward propagation...\n",
        "    elif direction.lower() == 'backward':\n",
        "      # return g'(z)\n",
        "      return None\n",
        "\n",
        "    else:\n",
        "      # otherwise, indicate invalid method call\n",
        "      raise Exception('Invalid acivation direction. Must be \"forward\" or \"backward\"')\n"
      ],
      "metadata": {
        "id": "Z5D0zYRqXQix"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function Tests - Layer Behaviour\n",
        "\n",
        "Below are some tests to ensure that the neural network layer produces the expected behaviour."
      ],
      "metadata": {
        "id": "sXRMB-A7YJjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new dense layer\n",
        "L1 = NN_DenseLayer(3,'Sigmoid','layer1')\n",
        "# create another dense layer with a bogus activation function\n",
        "L2 = NN_DenseLayer(3,'Bogus','layer2')"
      ],
      "metadata": {
        "id": "5_dkXBaq2Eva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print new layer\n",
        "print(L1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZGwoTaaJy8m",
        "outputId": "ffd6eea2-c0d8-4e4a-a5c3-d3977ccd9300"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Neural-network layer (dense) with 3 neurons. Sigmoid activation. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check layer weights\n",
        "L1.get_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvddf9FfJy4e",
        "outputId": "47fb36b4-5b21-4b7d-8753-ac62231e1a87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'W': None, 'B': None}"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the expected input shape\n",
        "L1.expectedInputShape"
      ],
      "metadata": {
        "id": "gSWzbGtuhv0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# activate the layer L1 by passing an array input\n",
        "test_input = np.array([[1,2,3]])\n",
        "L1.forward(test_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buRdiShgJywe",
        "outputId": "053b2603-e08f-40b0-c51e-132f37f06039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.50542075, 0.50019414, 0.52050466]])"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# summary of L2 layer\n",
        "print(L2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCFeWkPHp1kW",
        "outputId": "ce959902-1a6e-4927-d6b1-a03f494028e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Neural-network layer (dense) with 3 neurons. Bogus activation. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# attempt to activate L2 using the same input. input validation should catch bogus activation\n",
        "L2.forward(test_input)"
      ],
      "metadata": {
        "id": "t5zSIscxpQA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The expected input shape should now match the most recent input (1D array)\n",
        "L1.expectedInputShape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqOo470Qh3ch",
        "outputId": "3c0af4a0-e28a-49da-887a-4b3671b2b79e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check layer weights\n",
        "L1.get_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBq34AF2Y3wp",
        "outputId": "075af61d-47f8-4451-ea47-88941d78dd3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'W': array([[-0.00165777, -0.00157456,  0.00859604],\n",
              "        [-0.00418253, -0.0061997 , -0.00519123],\n",
              "        [ 0.01056889,  0.00491683,  0.02795036]]),\n",
              " 'B': array([[0., 0., 0.]])}"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reset weights\n",
        "W = None\n",
        "B = None\n",
        "L1.set_parameters(W, B)\n",
        "L1.get_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JPVVEb1ZFWt",
        "outputId": "4dc26096-e596-4fa3-cad2-1ca8d6e79f12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'W': None, 'B': None}"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set invalid weights\n",
        "W = np.arange(1,3,1)\n",
        "B = np.arange(1,10,2)\n",
        "L1.set_parameters(W, B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-R5OBmD8Zuad",
        "outputId": "e5d8c0e3-bba5-4cd3-bdb7-ac8ae4a0f53c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "INVALID COMMAND(!): \n",
            "\n",
            "Layer is only compatible with a (3, 3) weight matrix, and (1, 3) bias vector\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check that layer weights were NOT changed\n",
        "L1.get_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZVmLEXpgtvR",
        "outputId": "316136f9-f6f8-4f8a-dd3e-a2940612cdcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'W': None, 'B': None}"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# activate for a new input (matrix type)\n",
        "new_in = np.array([[1,2,3],[4,5,6]])\n",
        "L1.forward(new_in)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdCd89_cgwSy",
        "outputId": "a921915b-5d1c-41b3-adc9-987fd4d4799e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.49409322, 0.50071983, 0.49162361],\n",
              "       [0.49121147, 0.49958555, 0.48173364]])"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check layer weights\n",
        "L1.get_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17e_1u2QhDOS",
        "outputId": "7d4758ed-ab7a-4df8-9278-0f0f92f6be7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'W': array([[ 0.00820837, -0.00685627,  0.00587853],\n",
              "        [-0.00431803,  0.00629611, -0.01783766],\n",
              "        [-0.00773351, -0.00095221, -0.0012373 ]]),\n",
              " 'B': array([[0., 0., 0.]])}"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The expected input should update to match the new input type (2D matrix)\n",
        "L1.expectedInputShape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUvaNUJ1hOL7",
        "outputId": "1bab807a-1401-4021-c81d-375d56fc4b2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network Model Compiler"
      ],
      "metadata": {
        "id": "Kw-guNhXiujX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# enforce consistent results\n",
        "np.random.seed(1)\n",
        "\n",
        "class NN_Model:\n",
        "  # enforces consistent results\n",
        "  W_RAND =  [np.random.randn(3,2) * 0.01, np.random.randn(2,1) * 0.01]\n",
        "\n",
        "  def __init__(self,layers):\n",
        "    # assign layer identifiers for the model\n",
        "    self.layers = layers\n",
        "    self.num_layers = len(self.layers)\n",
        "    self.layer_activations = [layer.activation for layer in self.layers]\n",
        "    self.parameters = {}\n",
        "    self.current_input = None\n",
        "\n",
        "\n",
        "  def summarise(self):\n",
        "    '''\n",
        "    Summarises the features of the Neural Netwrok instance\n",
        "    '''\n",
        "    print(f\"{len(self.layers)} Layer Neural-Network (Sequential):\")\n",
        "    print(f\"------------------------------------\\n\")\n",
        "    for id in range(len(self.layers)):\n",
        "      print(f\"Layer {id + 1}\")\n",
        "      print(self.layers[id])\n",
        "\n",
        "\n",
        "  def count_features(self,X_in):\n",
        "    '''\n",
        "    Counts the number of features, n, present in the input X_in\n",
        "    '''\n",
        "    try:\n",
        "      # if X_in is a matrix, n is the number of columns, index 1\n",
        "      numFeatures = X_in.shape[1]\n",
        "    except IndexError:\n",
        "      # raise an error if the input is a 1D array\n",
        "      raise Exception('Input must be a 2D matrix.')\n",
        "\n",
        "    return numFeatures\n",
        "\n",
        "\n",
        "  def initialise_parameters(self, X_in):\n",
        "\n",
        "    # initialise the model dimensions starting with the input X_in\n",
        "    self.model_dimensions = [self.count_features(X_in)]\n",
        "\n",
        "    # loop through all layers in the model\n",
        "    for layer in range(self.num_layers):\n",
        "\n",
        "      # get the current layer\n",
        "      current_layer = self.layers[layer]\n",
        "\n",
        "      # add current layer dimensions to list\n",
        "      self.model_dimensions.append(current_layer.neurons)\n",
        "\n",
        "      # evaluate layer properties\n",
        "      W_temp, b_temp = current_layer.initialiseWB(self.model_dimensions[layer],self.model_dimensions[layer+1])\n",
        "      # assign weights and biases to a dictionary\n",
        "      self.parameters[\"W\" + str(layer+1)] = copy.deepcopy(self.W_RAND[layer]) # enforced results with W_RAND for testing\n",
        "      self.parameters[\"b\" + str(layer+1)] = copy.deepcopy(b_temp)\n",
        "\n",
        "\n",
        "  def get_parameters(self):\n",
        "    '''\n",
        "      Returns the models current weights and biases\n",
        "    '''\n",
        "    return self.parameters\n",
        "\n",
        "\n",
        "  def fit(self, X_in, Y_in):\n",
        "    '''\n",
        "      Computes a set of weight and bias matrices, W and B, for a given training dataset: X_in and Y_in, where\n",
        "      X_in is a set of input features\n",
        "      Y_in are the corresponding labels\n",
        "    '''\n",
        "    pass\n",
        "\n",
        "\n",
        "  def forward_propagation(self, X_in):\n",
        "    '''\n",
        "      Computes one full forward propagation pass on the NN given the input X_in. Returns the NN output/prediction\n",
        "      and a cache of internal layer inputs for the backwards propagation step.\n",
        "    '''\n",
        "\n",
        "    # initialise layer caches. these will store local input data for each layer\n",
        "    self.layer_caches = []\n",
        "\n",
        "    # initialise the current input\n",
        "    self.current_input = copy.deepcopy(X_in)\n",
        "\n",
        "    # loop through the entire network, and compute forward activations for each layer\n",
        "    for count, layer in enumerate(self.layers):\n",
        "\n",
        "      # extract the layer output and the cache of inputs\n",
        "      layer_output, layer_cache = layer.forward(self.current_input)\n",
        "      # append the input cache to a model instance\n",
        "      self.layer_caches.append(layer_cache)\n",
        "\n",
        "      # if the current layer is the last layer...\n",
        "      if count == self.num_layers - 1:\n",
        "        # ...then the output is our model prediction\n",
        "        self.model_prediction = layer_output\n",
        "\n",
        "      # for intermediate layers...\n",
        "      else:\n",
        "        # ...update current_input so that the next layer takes in the output of the current layer\n",
        "        self.current_input = layer_output\n",
        "\n",
        "    # return model output, and input logs\n",
        "    return self.model_prediction, self.layer_caches\n",
        "\n",
        "\n",
        "  def compute_cost(self, Y_in):\n",
        "    pass\n",
        "\n",
        "\n",
        "  def backward_propagation(self):\n",
        "    pass\n",
        "\n",
        "\n",
        "  def gradient_descent(self):\n",
        "    pass\n",
        "\n",
        "\n",
        "  def update_parameters(self):\n",
        "    pass\n",
        "\n",
        "\n",
        "  def predict(self):\n",
        "    pass\n",
        "\n"
      ],
      "metadata": {
        "id": "8D2pILctiZ5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function Tests - Model Behaviour"
      ],
      "metadata": {
        "id": "1Th5Qq6Dmfq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create NN layer objects\n",
        "L1 = NN_DenseLayer(2,'relu','layer1')\n",
        "L2 = NN_DenseLayer(1,'sigmoid','layer3')\n",
        "\n",
        "# pass layers to NN sequantial constructor model\n",
        "model = NN_Model([L1,L2])"
      ],
      "metadata": {
        "id": "pHRMZhZ4icuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "model.summarise()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXlogTcimlsI",
        "outputId": "24aedfa7-88ae-4459-87bd-f0f7bc6c7362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 Layer Neural-Network (Sequential):\n",
            "------------------------------------\n",
            "\n",
            "Layer 1\n",
            "\n",
            "Neural-network layer (dense) with 2 neurons. Relu activation. \n",
            "\n",
            "Layer 2\n",
            "\n",
            "Neural-network layer (dense) with 1 neurons. Sigmoid activation. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_input = np.random.randn(20,3)\n",
        "model.initialise_parameters(test_input)\n",
        "model.get_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-8Ky0_h9-q4",
        "outputId": "c935ed45-a7ca-4f87-9258-f823f6e51ae3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'W1': array([[ 0.01624345, -0.00611756],\n",
              "        [-0.00528172, -0.01072969],\n",
              "        [ 0.00865408, -0.02301539]]),\n",
              " 'b1': array([[0., 0.]]),\n",
              " 'W2': array([[ 0.01744812],\n",
              "        [-0.00761207]]),\n",
              " 'b2': array([[0.]])}"
            ]
          },
          "metadata": {},
          "execution_count": 229
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AL, _ = model.forward_propagation(test_input)\n",
        "print(AL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcSECRtdmyQO",
        "outputId": "72c15b15-392b-44b2-df89-5c9435becd21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.4999803 ]\n",
            " [0.50007778]\n",
            " [0.49996192]\n",
            " [0.50007881]\n",
            " [0.5       ]\n",
            " [0.5       ]\n",
            " [0.49999542]\n",
            " [0.49997661]\n",
            " [0.50001552]\n",
            " [0.49995607]\n",
            " [0.50002117]\n",
            " [0.50008683]\n",
            " [0.50001695]\n",
            " [0.50002047]\n",
            " [0.50015765]\n",
            " [0.49995681]\n",
            " [0.50000205]\n",
            " [0.50004541]\n",
            " [0.5001186 ]\n",
            " [0.5000574 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.array([[1,2],[3,4],[5,6]])**2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dd1JFDZwjMLf",
        "outputId": "de56c03f-e10b-48ff-d1d2-6ae9df582012"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  4],\n",
              "       [ 9, 16],\n",
              "       [25, 36]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sN1klu0lkw7n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}