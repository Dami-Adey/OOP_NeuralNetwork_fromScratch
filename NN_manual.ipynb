{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMcfLtK2Z6v7z+zwMixdmQq"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Neural Network from Scratch Using Object-Oriented Programming (OOP)\n",
        "\n",
        "In this notebook I develop an object-oriented sequential neural network from scratch as a personal project. The main motivator for this work is a more rigorous application of concepts learned in Andrew Ng's machine learning courses:\n",
        "\n",
        "- [Machine Learning Specialisation](https://www.coursera.org/specializations/machine-learning-introduction)\n",
        "- [Deep Learning Specialisation](https://www.coursera.org/specializations/deep-learning)\n",
        "\n",
        "\n",
        "The full implementation will have two main components:\n",
        "\n",
        "\n",
        "1.   A neural network layer constructor that allows the user to define layer properties similarly to the Keras API for TensorFlow.\n",
        "2.   A model compiler that takes a series of layers and forms the neural network.\n",
        "\n"
      ],
      "metadata": {
        "id": "wZkfvKDCMmVW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iY56k7IVWfPg"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network Layer Object\n",
        "\n",
        "The neural network layer object is created below. While the goal is mimic Keras' ease of use through OOP, the features included in this build are not an exhaustive reproduction of Keras' feature list, since that is well beyond the scope of this work. The following functionality  is included in the current version:\n",
        "- Generalised object architechture allows the user to initialise a layer with\n",
        "  - a number of units/neurons\n",
        "  - an activation function\n",
        "  - weight and bias parameters\n",
        "  - a layer id/name\n",
        "- A <u>print()</u> call produces a brief summary of the layer properties.\n",
        "- The <u>initialiseWB()</u> method initialises the layer weights and biases based on an expected input.\n",
        "- Once the model has been initialised for an input, the layer object validates any future inputs using the <u>is_consistent()</u> method.\n",
        "- The <u>get_parameters()</u> and <u>set_parameters()</u> methods allow for the inspection and  update of the layers weights and biases to facilitate the learning mechanisms.\n",
        "- The <u>activate()</u> method computes the local forward propagation step (vectorised) using one of the following activation functions:\n",
        "  - Linear (i.e, no activation function)\n",
        "  - Sigmoid\n",
        "  - Hyperbolic Tangent Function (tanh)\n",
        "  - Softmax\n",
        "  - Rectified Linear Unit (ReLU)\n"
      ],
      "metadata": {
        "id": "jjlAPoxQimtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NN_DenseLayer():\n",
        "  # class variables for scaling initial weights/biases\n",
        "  WB_SCALE = 0.01\n",
        "\n",
        "  def __init__(self,units,activation='sigmoid',name=None,W_init=None,B_init=None):\n",
        "    # initialise layer data\n",
        "    self.neurons = units          # number of neurons in the layer\n",
        "    self.activation = activation  # activation function applied at the layer\n",
        "    self.name = name              # layer name (if desired)\n",
        "\n",
        "    # features will be updated when an input is passed to the layer\n",
        "    self.features = None\n",
        "\n",
        "    # initialise layer weights and biases (inactive by default)\n",
        "    self.wb_initialised = False\n",
        "    self.W = W_init\n",
        "    self.B = B_init\n",
        "\n",
        "    # input sanitation parameter\n",
        "    self.expectedInputShape = None\n",
        "\n",
        "\n",
        "  def __str__(self):\n",
        "    '''\n",
        "      Prints a brief summary of the layer properties when print() is called.\n",
        "    '''\n",
        "    # print dialogue\n",
        "    return f\"\\nNeural-network layer (dense) with {self.neurons} neurons. {self.activation.title()} activation. \\n\"\n",
        "\n",
        "\n",
        "  def initialiseWB(self,X_in,featureCount,neuronCount):\n",
        "    '''\n",
        "      Produces initial values for weight and bias arrays:\n",
        "\n",
        "      PARAMETER INITIALISATION PROTOCOL:\n",
        "      --------------------------------------------------------------------------\n",
        "        - initialise the weights matrix to the desired (n,j), populating with a random sample from a normal distributiion\n",
        "        - initialise the bias matrix to the desired (1,j), populating with zeros\n",
        "        - scale both values using WB_SCALE\n",
        "    '''\n",
        "    # initialise weight array, and scale\n",
        "    weights_matrix = np.random.randn(featureCount,neuronCount) * self.WB_SCALE\n",
        "\n",
        "    # initialise bias array, and scale\n",
        "    bias_matrix = np.zeros((1,neuronCount)) * self.WB_SCALE\n",
        "\n",
        "    # indicate that weights/biases have been initialised\n",
        "    self.wb_initialised = True\n",
        "\n",
        "    return weights_matrix, bias_matrix\n",
        "\n",
        "\n",
        "  def is_consistent(self,X_in):\n",
        "    '''\n",
        "    Checks if the input X_in is consistent with previous inputs to the layer.\n",
        "    '''\n",
        "    # returns true if the input shape is consistent with the previous iterations\n",
        "    if X_in.shape == self.expectedInputShape:\n",
        "      return True\n",
        "    return False\n",
        "\n",
        "\n",
        "  def update_input(self, X_in):\n",
        "    '''\n",
        "      Updates the expected input shape for the layer based on the structure of a new input X_in\n",
        "    '''\n",
        "    # update the expected input shape to match X_in\n",
        "    self.expectedInputShape = X_in.shape\n",
        "\n",
        "\n",
        "\n",
        "  def count_features(self,X_in):\n",
        "    '''\n",
        "    Counts the number of features, n, present in the input X_in\n",
        "    '''\n",
        "    try:\n",
        "      # if X_in is a matrix, n is the number of columns, index 1\n",
        "      numFeatures = X_in.shape[1]\n",
        "    except IndexError:\n",
        "      # an IndexError above would suggest that X_in is a 1D array. Therefore n is index 0\n",
        "      numFeatures = X_in.shape[0]\n",
        "\n",
        "    return numFeatures\n",
        "\n",
        "\n",
        "  def activate(self, X_in):\n",
        "    '''\n",
        "      Evaluates the layer output(s) for an input X_in:\n",
        "\n",
        "      X_in -  input data    (m,n)  |  m examples with n features each\n",
        "      W    -  layer weights (n,j)  |  n features per neuron for j neurons/units\n",
        "      B    -  bias vector   (1,j)  |  j neurons/units\n",
        "    '''\n",
        "    self.current_input = X_in\n",
        "    # check if the layer was expecting an input of this format\n",
        "    if self.is_consistent(self.current_input):\n",
        "      pass # do nothing if input was expected in this format\n",
        "    else:\n",
        "      # we're dealing with a new input. so update the expected input shape\n",
        "      self.update_input(self.current_input)\n",
        "      # count the number of features in the new input format\n",
        "      self.features = self.count_features(self.current_input)\n",
        "      # initialise weight/bias arrays for the new input\n",
        "      self.W, self.B = self.initialiseWB(self.current_input,self.features,self.neurons)\n",
        "\n",
        "    # if weights/biases have not yet been initialised, then do so.\n",
        "    if not self.wb_initialised:\n",
        "      # estimate inital values for W and B\n",
        "      self.W, self.B = self.initialiseWB(self.current_input,self.features,self.neurons)\n",
        "\n",
        "    # apply weights an biases to inputs; (z = X_in*W + B)\n",
        "    z = np.matmul(self.current_input, self.W) + self.B\n",
        "\n",
        "    # library of built-in activation functions\n",
        "    activation_library = {\n",
        "        'linear':self.linear(z),\n",
        "        'sigmoid':self.sigmoid(z),\n",
        "        'tanh':self.tanh(z),\n",
        "        'softmax':self.softmax(z),\n",
        "        'relu':self.relu(z)\n",
        "    }\n",
        "\n",
        "    # attempt to apply the chosen activation function\n",
        "    try:\n",
        "      # pull the relevant activation output\n",
        "      self.out = activation_library[self.activation.lower()]\n",
        "    except KeyError:\n",
        "      # activation call failed\n",
        "      self.out = None\n",
        "      # find valid activation functions\n",
        "      valid_activation_keys = [key for (key, values) in activation_library.items()]\n",
        "      # throw exception and recommend valid activation functions\n",
        "      raise Exception(f\"Invalid activation function. Pick one of the following: {valid_activation_keys}\")\n",
        "\n",
        "    return self.out\n",
        "\n",
        "\n",
        "  def get_parameters(self):\n",
        "    '''\n",
        "      Returns the current weights and biases for the layer\n",
        "    '''\n",
        "    # package parameters into a dictionary and return\n",
        "    self.parameters = {'W':self.W, 'B':self.B}\n",
        "    return self.parameters\n",
        "\n",
        "\n",
        "  def set_parameters(self,W_set,B_set):\n",
        "    '''\n",
        "      Overwrites the weights and biases of the current layer\n",
        "    '''\n",
        "    try:\n",
        "      # if setpoint values are null, then W/B are set to None\n",
        "      if (W_set == None) and (B_set == None):\n",
        "        self.W = W_set\n",
        "        self.B = B_set\n",
        "        # indicate that the weights/biases have been nullified\n",
        "        self.wb_initialised = False\n",
        "\n",
        "    except ValueError:\n",
        "      if (self.W == None) or (self.B == None):\n",
        "        # create proxy weights/biases using the current input structure\n",
        "        W_proxy, B_proxy = self.initialiseWB(self.current_input,self.features,self.neurons)\n",
        "      else:\n",
        "        W_proxy = self.W\n",
        "        B_proxy = self.B\n",
        "\n",
        "      # if non-null, the shape of the weight and bias structures should be consistent\n",
        "      if (W_set.shape == W_proxy.shape) and (B_set.shape == B_proxy.shape):\n",
        "        self.W = W_set\n",
        "        self.B = B_set\n",
        "      # if not, the W/B structures chosen are incompatible\n",
        "      else:\n",
        "        return print(f\"\\nINVALID COMMAND(!): \\n\\nLayer is only compatible with a {W_proxy.shape} weight matrix, and {B_proxy.shape} bias matrix\")\n",
        "\n",
        "\n",
        "  #----------------------------------------------------------------------------#\n",
        "  # ACTIVATION FUNCTIONS\n",
        "  #----------------------------------------------------------------------------#\n",
        "  # Linear function (regression problems; negative or positive)\n",
        "  def linear(self,z):\n",
        "    '''\n",
        "      Evaluates the linear function g on an input z\n",
        "\n",
        "      Such that, g(z) = z\n",
        "    '''\n",
        "    return z\n",
        "\n",
        "\n",
        "  # Sigmoid function (binary classification problems; 1 or 0, etc.)\n",
        "  def sigmoid(self,z):\n",
        "    '''\n",
        "      Evaluates the sigmoid function g on an input z\n",
        "\n",
        "      Where, g(z) =      1\n",
        "                     ----------\n",
        "                     1 + e^(-z)\n",
        "    '''\n",
        "    return 1/(1 + np.exp(-z))\n",
        "\n",
        "\n",
        "  # Hyperbolic Tangent function (alternative to sigmoid, varies between -1 an 1 instead of 0 and 1)\n",
        "  def tanh(self,z):\n",
        "    '''\n",
        "      Evaluates the Hyperbolic Tangent (tanh) function g on an input z\n",
        "\n",
        "      Where, g(z) =    e^(z) - e^(-z)\n",
        "                       --------------\n",
        "                       e^(z) + e^(-z)\n",
        "    '''\n",
        "    return np.tanh(z)\n",
        "\n",
        "\n",
        "  # ReLU function (regression problems; threshold utility; non-negative values only)\n",
        "  def relu(self,z):\n",
        "    '''\n",
        "      Evaluates the Rectified Linear Unit (ReLU) function g on an input z\n",
        "\n",
        "      Where, g(z) = 0, if (z < 0)\n",
        "             g(z) = z, if (z > 0)\n",
        "    '''\n",
        "    return np.maximum(0,z)\n",
        "\n",
        "\n",
        "  # Softmax activation (multi-class classification, i.e. mutiple dicrete choices, one correct choice per example)\n",
        "  def softmax(self,z):\n",
        "    '''\n",
        "    Evaluates the softmax activation function g on an input z. Softmax converts\n",
        "    z for all possible choice into a distribution of probabilities that each\n",
        "    choice is correct.\n",
        "\n",
        "    For a problem with N choices, the probability that the kth choice is\n",
        "    correct is represented by:\n",
        "\n",
        "           g(z_{i,j,k,...,N}) =         e^z_k\n",
        "                                 -------------------\n",
        "                                 e^z_i + ... + e^z_N\n",
        "    '''\n",
        "    return np.exp(z)/np.sum(np.exp(z))\n"
      ],
      "metadata": {
        "id": "Z5D0zYRqXQix"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function Tests - Layer Behaviour\n",
        "\n",
        "Below are some tests to ensure that the neural network layer produces the expected behaviour."
      ],
      "metadata": {
        "id": "sXRMB-A7YJjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new dense layer\n",
        "L1 = NN_DenseLayer(3,'Sigmoid','layer1')\n",
        "# create another dense layer with a bogus activation function\n",
        "L2 = NN_DenseLayer(3,'Bogus','layer2')"
      ],
      "metadata": {
        "id": "5_dkXBaq2Eva"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print new layer\n",
        "print(L1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZGwoTaaJy8m",
        "outputId": "ffd6eea2-c0d8-4e4a-a5c3-d3977ccd9300"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Neural-network layer (dense) with 3 neurons. Sigmoid activation. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check layer weights\n",
        "L1.get_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvddf9FfJy4e",
        "outputId": "47fb36b4-5b21-4b7d-8753-ac62231e1a87"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'W': None, 'B': None}"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the expected input shape\n",
        "L1.expectedInputShape"
      ],
      "metadata": {
        "id": "gSWzbGtuhv0B"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# activate the layer L1 by passing an array input\n",
        "test_input = np.array([[1,2,3]])\n",
        "L1.activate(test_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buRdiShgJywe",
        "outputId": "053b2603-e08f-40b0-c51e-132f37f06039"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.50542075, 0.50019414, 0.52050466]])"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# summary of L2 layer\n",
        "print(L2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCFeWkPHp1kW",
        "outputId": "ce959902-1a6e-4927-d6b1-a03f494028e0"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Neural-network layer (dense) with 3 neurons. Bogus activation. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# attempt to activate L2 using the same input. input validation should catch bogus activation\n",
        "L2.activate(test_input)"
      ],
      "metadata": {
        "id": "t5zSIscxpQA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The expected input shape should now match the most recent input (1D array)\n",
        "L1.expectedInputShape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqOo470Qh3ch",
        "outputId": "3c0af4a0-e28a-49da-887a-4b3671b2b79e"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check layer weights\n",
        "L1.get_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBq34AF2Y3wp",
        "outputId": "075af61d-47f8-4451-ea47-88941d78dd3d"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'W': array([[-0.00165777, -0.00157456,  0.00859604],\n",
              "        [-0.00418253, -0.0061997 , -0.00519123],\n",
              "        [ 0.01056889,  0.00491683,  0.02795036]]),\n",
              " 'B': array([[0., 0., 0.]])}"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reset weights\n",
        "W = None\n",
        "B = None\n",
        "L1.set_parameters(W, B)\n",
        "L1.get_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JPVVEb1ZFWt",
        "outputId": "4dc26096-e596-4fa3-cad2-1ca8d6e79f12"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'W': None, 'B': None}"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set invalid weights\n",
        "W = np.arange(1,3,1)\n",
        "B = np.arange(1,10,2)\n",
        "L1.set_parameters(W, B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-R5OBmD8Zuad",
        "outputId": "e5d8c0e3-bba5-4cd3-bdb7-ac8ae4a0f53c"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "INVALID COMMAND(!): \n",
            "\n",
            "Layer is only compatible with a (3, 3) weight matrix, and (1, 3) bias vector\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check that layer weights were NOT changed\n",
        "L1.get_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZVmLEXpgtvR",
        "outputId": "316136f9-f6f8-4f8a-dd3e-a2940612cdcd"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'W': None, 'B': None}"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# activate for a new input (matrix type)\n",
        "new_in = np.array([[1,2,3],[4,5,6]])\n",
        "L1.activate(new_in)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdCd89_cgwSy",
        "outputId": "a921915b-5d1c-41b3-adc9-987fd4d4799e"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.49409322, 0.50071983, 0.49162361],\n",
              "       [0.49121147, 0.49958555, 0.48173364]])"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check layer weights\n",
        "L1.get_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17e_1u2QhDOS",
        "outputId": "7d4758ed-ab7a-4df8-9278-0f0f92f6be7e"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'W': array([[ 0.00820837, -0.00685627,  0.00587853],\n",
              "        [-0.00431803,  0.00629611, -0.01783766],\n",
              "        [-0.00773351, -0.00095221, -0.0012373 ]]),\n",
              " 'B': array([[0., 0., 0.]])}"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The expected input should update to match the new input type (2D matrix)\n",
        "L1.expectedInputShape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUvaNUJ1hOL7",
        "outputId": "1bab807a-1401-4021-c81d-375d56fc4b2c"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network Model Compiler"
      ],
      "metadata": {
        "id": "Kw-guNhXiujX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NN_Model:\n",
        "  def __init__(self,layers):\n",
        "    # assign layer identifiers for the NN\n",
        "    self.layers = layers\n",
        "    self.num_layers = len(self.layers)\n",
        "    self.current_input = None\n",
        "\n",
        "\n",
        "  def summarise(self):\n",
        "    '''\n",
        "    Summarises the features of the Neural Netwrok instance\n",
        "    '''\n",
        "    print(f\"{len(self.layers)} Layer Neural-Network (Sequential):\")\n",
        "    print(f\"------------------------------------\\n\")\n",
        "    for id in range(len(self.layers)):\n",
        "      print(f\"Layer {id + 1}\")\n",
        "      print(self.layers[id])\n",
        "\n",
        "\n",
        "  def compile(self):\n",
        "    pass\n",
        "\n",
        "\n",
        "  def fit(self):\n",
        "    pass\n",
        "\n",
        "\n",
        "  def forward_propagation(self):\n",
        "    pass\n",
        "\n",
        "\n",
        "  def backward_propagation(self):\n",
        "    pass\n",
        "\n",
        "\n",
        "  def gradient_descent(self):\n",
        "    pass\n",
        "\n",
        "\n",
        "  def update_parameters(self):\n",
        "    pass\n",
        "\n",
        "\n",
        "  def predict(self):\n",
        "    pass\n",
        "\n"
      ],
      "metadata": {
        "id": "8D2pILctiZ5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unit Tests - Sequential NN Behaviour"
      ],
      "metadata": {
        "id": "1Th5Qq6Dmfq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create NN layer objects\n",
        "L1 = NN_DenseLayer(25,'sigmoid','layer1')\n",
        "L2 = NN_DenseLayer(15,'sigmoid','layer2')\n",
        "L3 = NN_DenseLayer(1,'sigmoid','layer3')\n",
        "\n",
        "# pass layers to NN sequantial constructor model\n",
        "model = NN_Model([L1,L2,L3])"
      ],
      "metadata": {
        "id": "pHRMZhZ4icuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "model.summarise()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXlogTcimlsI",
        "outputId": "a4fded27-7b16-4795-af21-b1fc746f361c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 Layer Neural-Network (Sequential):\n",
            "------------------------------------\n",
            "\n",
            "Layer 1\n",
            "\n",
            "Neural-network layer (dense) with 25 neurons. Sigmoid activation. \n",
            "\n",
            "Layer 2\n",
            "\n",
            "Neural-network layer (dense) with 15 neurons. Sigmoid activation. \n",
            "\n",
            "Layer 3\n",
            "\n",
            "Neural-network layer (dense) with 1 neurons. Sigmoid activation. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5PnP53UkLWJd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}